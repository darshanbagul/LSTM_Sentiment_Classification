{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Sentiment Analysis\n",
    "\n",
    "This notebook aims to provide an example of how a Recurrent Neural Network (RNN) using the Long Short Term Memory (LSTM) architecture can be implemented using Keras(https://keras.io/). Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. \n",
    "\n",
    "In this notebook, we have experimented with LSTM to perform sentiment analysis on movie reviews from the Large Movie Review Dataset(http://ai.stanford.edu/~amaas/data/sentiment/), better known as the IMDB dataset.\n",
    "\n",
    "In this task, given a movie review, the model attempts to predict whether it is positive or negative. This is a binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "As previously mentioned, we shall train a LSTM recurrent neural network on the Large Movie Review Dataset dataset.\n",
    "\n",
    "The Large Movie Review Dataset (often referred to as the IMDB dataset) contains 25,000 highly-polar movie reviews (good or bad) for training and the same amount again for testing. The problem is to determine whether a given movie review has a positive or negative sentiment.\n",
    "\n",
    "The data was collected by Stanford researchers and was used in a 2011 paper where a split of 50-50 of the data was used for training and test. **An accuracy of 88.89% was achieved.**\n",
    "\n",
    "### Keras advantages:\n",
    "As stated earlier, Keras was built with a focus on fast experimentation and prototyping. Hence,Keras provides access to the IMDB dataset built-in! \n",
    "\n",
    "The **imdb.load_data()** function allows you to load the dataset in a format that is ready for use in neural network and deep learning models.\n",
    "\n",
    "The words have been replaced by integers that indicate the ordered frequency of each word in the dataset. The sentences in each review are therefore comprised of a sequence of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "The vocabulary of words in all the reviews is very large. Mere one-hot encoding of individual words will lead to an extremely sparse dataset.\n",
    "\n",
    "Hence we will use Word Embeddings, a technique where words are encoded as real-valued vectors in a high dimensional space, where the similarity between words in terms of meaning translates to closeness in the vector space. This reduces sparsity of the data, as well as gives more meaning to each word in this embedded space, rather than just present or not. For more information on word embeddings, check out my experiments with Word Embeddings on Harry Potter and Game of Thrones [here](https://github.com/darshanbagul/Word_Embeddings)!\n",
    "\n",
    "In this notebook we won't have to use Gensim or create an embedding network from scratch. This is another advantage of using Keras. We just include another layer after the input in our model for generating embeddings of the input word! Keras provides a convenient way to convert positive integer representations of words into a word embedding by an **Embedding layer.**\n",
    "\n",
    "We will map each word onto a **32 length real valued vector**. We will also limit the total number of words that we are interested in modeling to the **5000 most frequent words, and zero out the rest.** Finally, the sequence length (number of words) in each review varies, so we will **constrain each review to be 500 words,** truncating long reviews and pad the shorter reviews with zero values. (Another alternative for this could be experimenting with a Dynamic RNN, but that is something I shall experiment with later.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "Now that we have preprocessed the dataset and split into train and test, let us begin exploring with different model architectures. The models that we are going to implement are listed below with increasing complexity:\n",
    "\n",
    "    1. A Simple LSTM Network\n",
    "    2. LSTM Network with Dropout regularization\n",
    "    3. CNN + LSTM with Dropout regularization\n",
    "\n",
    "Let us dive right in with each model!\n",
    "\n",
    "### 1. A Simple LSTM Network\n",
    "\n",
    "We can quickly develop a small LSTM for the IMDB sentiment classification and achieve good accuracy as we will see below. The model architecture is printed after defining the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 988s - loss: 0.5734 - acc: 0.6856   \n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 991s - loss: 0.3090 - acc: 0.8736   \n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 1325s - loss: 0.2415 - acc: 0.9054  \n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 1662s - loss: 0.2174 - acc: 0.9150  \n",
      "Accuracy: 85.72%\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=4, batch_size=64)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LSTM Network With Dropout\n",
    "\n",
    "RNNs like LSTM generally suffer from overfitting the data. Dropout is a technique of countering overfitting by improving the ability of model to generalise better. This is done by setting random activations of hidden layers to zero. Through this randomness, the final classification layer always receives a different transformed input for a corresponding output, thus preventing overfitting and improving model's ability to generalize on unseen data. \n",
    "\n",
    "Dropout can be applied between layers using the Dropout Keras layer. We can do this easily by \n",
    "    1. Adding new Dropout layers between the Embedding and LSTM layers and the LSTM and Dense output layers, or \n",
    "    2. As parameters on the LSTM layer; the 'dropout' for configuring the input dropout and 'recurrent_dropout' for configuring the recurrent dropout.\n",
    "    \n",
    "Here we will be using the latter option, just for the sake of a compact solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model architecture looks as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 1675s - loss: 0.5034 - acc: 0.7552  \n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 1707s - loss: 0.3894 - acc: 0.8316  \n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 4068s - loss: 0.3404 - acc: 0.8586  \n",
      "Accuracy: 85.67%\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model accuracy took a hit, but the model generalizes better. This model is more robust to noise and changes in data representations, as compared to the previous model. \n",
    "\n",
    "Dropout is a powerful technique for combating overfitting in LSTM models!\n",
    "\n",
    "Let us now try and combine Convolutional Neural Networks along with LSTM to check if we are able to increase the model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LSTM and Convolutional Neural Network with dropout\n",
    "\n",
    "Convolutional neural networks (CNNs) generally excel at learning the spatial structure in input data. Hence they are widely used on data which comprise of highly correlated spatial structures. For example - Images! Images are unstructured data points, where groups of pixels represent a particular structure. Presence or absence of such structures, helps us classify images into particular category.\n",
    "\n",
    "The IMDB review data does have a one-dimensional spatial structure in the sequence of words in reviews and the CNN may be able to pick out invariant features for good and bad sentiment. This learned spatial features may then be learned as sequences by an LSTM layer.\n",
    "\n",
    "Adding a 1-D Convolutional layer followed by Max pooling is elementary in Keras. \n",
    "\n",
    "Here we will add a 1-D Conv layer and max pooling layer after the Embedding layer which then feed the consolidated features to the LSTM. We use a set of 32 feature maps (convolutional filters) with a size of 3x3. The pooling layer can use the standard length of 2 to halve the feature map size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the model architecture below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 530s - loss: 0.4880 - acc: 0.7500   \n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 501s - loss: 0.3197 - acc: 0.8676   \n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 544s - loss: 0.2695 - acc: 0.8927   \n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 557s - loss: 0.2383 - acc: 0.9053   \n",
      "Accuracy: 88.10%\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=4, batch_size=64)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 561s - loss: 0.1997 - acc: 0.9222   \n",
      "Accuracy: 88.18%\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=1, batch_size=64)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "As we can see, with a mixture of a **Convolutional Neural network + LSTM**, we are able to achieve an accuracy as high as the state-of-art accuracy for this dataset. As stated earlier, the Stanford researchers were able to achieve an accuracy of 88.89%, and **we have been able to reach 88.18%** (I even achieved **88.41%** with a different seed earlier!)\n",
    "\n",
    "Some reflections:\n",
    "    1. CNN and max pooling layers after the Embedding layer are able to pick out invariant features for good and bad sentiment. These learned spatial features are then learned as sequences by an LSTM layer.\n",
    "    2. We have less weights and faster training time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we implemented LSTM network models for sequence classification problem, specifically sentiment classification of movie reviews from **Large Movie Review Dataset dataset.**\n",
    "\n",
    "    1. Implemented a simple single layer LSTM model for the IMDB movie review sentiment classification problem.\n",
    "    2. Extended LSTM model with LSTM-specific dropout to combat overfitting.\n",
    "    3. Combined the spatial structure learning properties of a Convolutional Neural Network (CNN) with the sequence learning of an LSTM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
